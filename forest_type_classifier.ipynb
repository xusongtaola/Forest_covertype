{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook,  we test a neural netowrk as a model to predict the forest cover type from cartographic variables.\n",
    "\n",
    "\n",
    "The data was obtained from the UCI machine learning repository, The data consists of 54 features and 581012 instances. As specified at https://archive.ics.uci.edu/ml/datasets/Covertype, the variables are\n",
    "\n",
    "Name / Data Type / Measurement / Description\n",
    "\n",
    "Elevation / quantitative /meters / Elevation in meters\n",
    "\n",
    "Aspect / quantitative / azimuth / Aspect in degrees azimuth\n",
    "\n",
    "Slope / quantitative / degrees / Slope in degrees\n",
    "\n",
    "Horizontal_Distance_To_Hydrology / quantitative / meters / Horz Dist to nearest surface water features\n",
    "\n",
    "Vertical_Distance_To_Hydrology / quantitative / meters / Vert Dist to nearest surface water features\n",
    "\n",
    "Horizontal_Distance_To_Roadways / quantitative / meters / Horz Dist to nearest roadway\n",
    "\n",
    "Hillshade_9am / quantitative / 0 to 255 index / Hillshade index at 9am, summer solstice\n",
    "\n",
    "Hillshade_Noon / quantitative / 0 to 255 index / Hillshade index at noon, summer soltice\n",
    "\n",
    "Hillshade_3pm / quantitative / 0 to 255 index / Hillshade index at 3pm, summer solstice\n",
    "\n",
    "Horizontal_Distance_To_Fire_Points / quantitative / meters / Horz Dist to nearest wildfire ignition points\n",
    "\n",
    "Wilderness_Area (4 binary columns) / qualitative / 0 (absence) or 1 (presence) / Wilderness area designation\n",
    "\n",
    "Soil_Type (40 binary columns) / qualitative / 0 (absence) or 1 (presence) / Soil Type designation\n",
    "\n",
    "Cover_Type (7 types) / integer / 1 to 7 / Forest Cover Type designation  (response variable)\n",
    "\n",
    "The forest cover type classes are:\n",
    "\n",
    "Forest Cover Type Classes:\t    1 -- Spruce/Fir\n",
    "\n",
    "                                2 -- Lodgepole Pine\n",
    "                                \n",
    "                                3 -- Ponderosa Pine\n",
    "                                \n",
    "                                4 -- Cottonwood/Willow\n",
    "                                \n",
    "                                5 -- Aspen\n",
    "                                \n",
    "                                6 -- Douglas-fir\n",
    "                                \n",
    "                                7 -- Krummholz\n",
    "\n",
    "and the the number of instances for each class is\n",
    "\n",
    "           Number of records of Spruce-Fir:                211840 \n",
    "           \n",
    "           Number of records of Lodgepole Pine:            283301 \n",
    "           \n",
    "           Number of records of Ponderosa Pine:             35754 \n",
    "           \n",
    "           Number of records of Cottonwood/Willow:           2747 \n",
    "           \n",
    "           Number of records of Aspen:                       9493 \n",
    "           \n",
    "           Number of records of Douglas-fir:                17367 \n",
    "           \n",
    "           Number of records of Krummholz:                  20510  \n",
    "           \n",
    "          \t\n",
    "           Total records:                                  581012\n",
    "\n",
    "As we can see, the first two classes make up more than 85% of our data. To simplify things, we will not use all of the data. For the first two classes, we will randomly choose 35700 instances and ignore data related to the Cottonwood/Willow and Aspen classes and thus work with a data set of 5 classes.\n",
    "\n",
    "We use the Neural network class implementation from our package ML_tools. We use the update method ADAM, a stochastic gradient-based method introduced in http://arxiv.org/pdf/1412.6980.pdf to train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env/python\n",
    "from __future__ import print_function \n",
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pickle\n",
    "import urllib.request\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving and uncompressing file covtype.dat\n",
      "Compressed file succesfully retrieved\n",
      "\n",
      "Extracting file:\n",
      "File succesfully extracted\n"
     ]
    }
   ],
   "source": [
    "#Retrieving data\n",
    "current = os.getcwd() #current directory\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz'  #link to zip file\n",
    "filename = 'covtype.dat'\n",
    "fullpath = os.path.join(current,filename)\n",
    "if  filename in os.listdir(current):\n",
    "    print('Data file %s already present' %filename)\n",
    "else:\n",
    "    #Attempt to retrieve compressed file\n",
    "    print('Retrieving and uncompressing file %s' %filename)\n",
    "    try:\n",
    "        zipped_fullpath = fullpath + '.gz'\n",
    "        if sys.version_info[0] >= 3:  #for Python 3\n",
    "            zipped_fullpath, _ = urllib.request.urlretrieve(url, zipped_fullpath)\n",
    "        else: #for Python 2\n",
    "            zipped_fullpath, _ = urllib.urlretrieve(url, zipped_fullpath)\n",
    "        print('Compressed file succesfully retrieved')\n",
    "    except IOError as e:\n",
    "        print('Cannot retrieve %s from %s: s'% (url,zipped_fullpath, e))\n",
    "    #Attempt to extract file\n",
    "    print('\\nExtracting file:')\n",
    "    try:\n",
    "        with gzip.open(zipped_fullpath,'rb') as file:\n",
    "            s = file.read()\n",
    "            file.close()\n",
    "        with open(fullpath,'wb') as f:\n",
    "            f.write(s)\n",
    "        print('File succesfully extracted')\n",
    "    except IOError as e:\n",
    "        print('Could not decompress %s: s' (zipped_fullpath, e))\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next,the data is loaded and we normalize the quantitative variables. The hillshade variables are index varaiables taking values between 0 and 255. These variables are linearly rescaled to take values between -1 and 1. The other quantitative variables are normalized the usual way by considering the mean and standard deviation over the whole dataset. We partition the data into a training set and a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def relabel(y):\n",
    "    #function used for relabelling our class labels\n",
    "    if y < 5:\n",
    "        return y-1\n",
    "    else:\n",
    "        return y-3\n",
    "vecrelabel = np.vectorize(relabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing, partitioning and storing the data\n",
      "Shape of Data_loaded: (581012, 55)\n",
      "Shape of data used:  (145031, 55)\n",
      "Data_train shape: (79767, 55)\n",
      "Data_val shape: (65264, 55)\n"
     ]
    }
   ],
   "source": [
    "#Loading file as numpy array, preprocessing, partitioning data into training and validation datasets and finally\n",
    "#storing the preprocessed data as binary file if the file is not present already\n",
    "filename = 'covtype_preprocess.dat'\n",
    "os.path.join(current,filename)\n",
    "if filename in os.listdir(current):\n",
    "    print('Preprocessed data already present.')\n",
    "    print('Retrieving numpy arrays')\n",
    "    f = open(filename,'rb')\n",
    "    Data_train  = pickle.load(f)\n",
    "    Data_val = pickle.load(f)\n",
    "    mean = pickle.load(f)\n",
    "    std = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    print('Preprocessing, partitioning and storing the data')\n",
    "    Data_loaded  = np.loadtxt('covtype.dat', delimiter =',')\n",
    "    print('Shape of Data_loaded:',Data_loaded.shape)\n",
    "    #Selecting Data from Data_loaded:\n",
    "    #1) Randomly choosing 35700 instances for the first two classes\n",
    "    spruce_condition = (Data_loaded[:,-1] == 1)   \n",
    "    Data_spruce = Data_loaded[spruce_condition,:]   #data for first class\n",
    "    \n",
    "    selection = np.random.choice(len(Data_spruce), 35700)\n",
    "    Data_sprucesl = Data_spruce[selection,:]  #selection of instances\n",
    "    \n",
    "    \n",
    "    lodgepole_condition = (Data_loaded[:,-1] == 2)   \n",
    "    Data_lodgepole = Data_loaded[lodgepole_condition,:] #data for second class\n",
    "    \n",
    "    selection = np.random.choice(len(Data_lodgepole), 35700)\n",
    "    Data_lodgepolesl = Data_lodgepole[selection,:]  #selection of instances for second class\n",
    "   \n",
    "    \n",
    "    #rest of data\n",
    "    willow_condition = (Data_loaded[:,-1] == 4)\n",
    "    aspen_condition =  (Data_loaded[:,-1] == 5)\n",
    "    condition3 = np.logical_or(willow_condition, aspen_condition)  #aspen or will type\n",
    "    condition = np.logical_not(np.logical_or(np.logical_or(spruce_condition,lodgepole_condition),condition3))\n",
    "    Data_other = Data_loaded[condition,:]\n",
    "    \n",
    "    Data = np.concatenate((Data_other, Data_sprucesl,Data_lodgepolesl),axis=0) #data used\n",
    "    print('Shape of data used: ',Data.shape)\n",
    "    \n",
    "    #Normalizing the data and relabelling the classes\n",
    "    mean = np.mean(Data[:,[0,1,2,3,4,5,9]], axis = 0)\n",
    "    std  = np.std(Data[:,[0,1,2,3,4,5,9]], axis = 0)\n",
    "    Data[:,[0,1,2,3,4,5,9]] = (Data[:,[0,1,2,3,4,5,9]]-mean)/std\n",
    "    Data[:,6:9] = 2.0*Data[:,6:9]/255.0 -1\n",
    "    Data[:,-1] = vecrelabel(Data[:,-1]) \n",
    "    #Partitioning the data\n",
    "    N = len(Data)\n",
    "    u = np.random.permutation(np.arange(N))\n",
    "    N_train = int(round(0.55*N))\n",
    "    Data_train = Data[u[:N_train],:]\n",
    "    Data_val = Data[u[N_train:],:]\n",
    "   \n",
    "    #Saving the data\n",
    "    f = open(filename,'wb')\n",
    "    pickle.dump(Data_train,f)\n",
    "    pickle.dump(Data_val,f)\n",
    "    pickle.dump(mean,f)\n",
    "    pickle.dump(std,f)\n",
    "    f.close()\n",
    "\n",
    "print('Data_train shape:',Data_train.shape)\n",
    "print('Data_val shape:',Data_val.shape)\n",
    "  \n",
    "  \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79767, 54)\n",
      "(65264, 54)\n"
     ]
    }
   ],
   "source": [
    "#Preparing data for training. \n",
    "Xtr = Data_train[:,:-1]\n",
    "print(Xtr.shape)\n",
    "Ytr = Data_train[:,-1]\n",
    "Ytr = Ytr.astype(int)\n",
    "\n",
    "Xv  = Data_val[:,:-1]\n",
    "print(Xv.shape)\n",
    "Yv = Data_val[:,-1]\n",
    "Yv = Yv.astype(int)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading Neural_network module\n",
    "#Assuming ML_tools package is in samed directory containing covertype directory\n",
    "containing_directory = os.path.split(current)[0]\n",
    "fullpath = os.path.join(containing_directory,'ML_tools') #path to ML_tools package\n",
    "sys.path.append(fullpath)\n",
    "from Neural_network import NeuralNetwork\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 0 / 30) training accuracy: 43.9266864744; validation accuracy: 43.9108850208\n",
      "Iteration 50 / 23910) loss: 0.64427259922\n",
      "Iteration 100 / 23910) loss: 0.761224580288\n",
      "Iteration 150 / 23910) loss: 0.753147710085\n",
      "Iteration 200 / 23910) loss: 0.724600511551\n",
      "Iteration 250 / 23910) loss: 0.785159703732\n",
      "Iteration 300 / 23910) loss: 0.664028719425\n",
      "Iteration 350 / 23910) loss: 0.825770318985\n",
      "Iteration 400 / 23910) loss: 0.586271295071\n",
      "Iteration 450 / 23910) loss: 0.752010461807\n",
      "Iteration 500 / 23910) loss: 0.604125889301\n",
      "Iteration 550 / 23910) loss: 0.614490700722\n",
      "Iteration 600 / 23910) loss: 0.741288555622\n",
      "Iteration 650 / 23910) loss: 0.703974433422\n",
      "Iteration 700 / 23910) loss: 0.594110956192\n",
      "Iteration 750 / 23910) loss: 0.704151956081\n",
      "(Epoch 1 / 30) training accuracy: 73.8651321975; validation accuracy: 74.1250919343\n",
      "Iteration 800 / 23910) loss: 0.670640815735\n",
      "Iteration 850 / 23910) loss: 0.604857423782\n",
      "Iteration 900 / 23910) loss: 0.581287454605\n",
      "Iteration 950 / 23910) loss: 0.752587881088\n",
      "Iteration 1000 / 23910) loss: 0.594463537693\n",
      "Iteration 1050 / 23910) loss: 0.749670690536\n",
      "Iteration 1100 / 23910) loss: 0.563048345089\n",
      "Iteration 1150 / 23910) loss: 0.66545686388\n",
      "Iteration 1200 / 23910) loss: 0.535199112892\n",
      "Iteration 1250 / 23910) loss: 0.636176352024\n",
      "Iteration 1300 / 23910) loss: 0.787997263908\n",
      "Iteration 1350 / 23910) loss: 0.672359577179\n",
      "Iteration 1400 / 23910) loss: 0.646710684776\n",
      "Iteration 1450 / 23910) loss: 0.718778105259\n",
      "Iteration 1500 / 23910) loss: 0.596305635452\n",
      "Iteration 1550 / 23910) loss: 0.623117143631\n",
      "(Epoch 2 / 30) training accuracy: 75.1275590156; validation accuracy: 75.2359646972\n",
      "Iteration 1600 / 23910) loss: 0.571571624756\n",
      "Iteration 1650 / 23910) loss: 0.613126094341\n",
      "Iteration 1700 / 23910) loss: 0.627557230949\n",
      "Iteration 1750 / 23910) loss: 0.783941899776\n",
      "Iteration 1800 / 23910) loss: 0.618349378586\n",
      "Iteration 1850 / 23910) loss: 0.607489148617\n",
      "Iteration 1900 / 23910) loss: 0.610192399502\n",
      "Iteration 1950 / 23910) loss: 0.607163599968\n",
      "Iteration 2000 / 23910) loss: 0.678377007484\n",
      "Iteration 2050 / 23910) loss: 0.722682506561\n",
      "Iteration 2100 / 23910) loss: 0.587366856098\n",
      "Iteration 2150 / 23910) loss: 0.510167948246\n",
      "Iteration 2200 / 23910) loss: 0.619471579075\n",
      "Iteration 2250 / 23910) loss: 0.621451499939\n",
      "Iteration 2300 / 23910) loss: 0.621843719482\n",
      "Iteration 2350 / 23910) loss: 0.649836131096\n",
      "(Epoch 3 / 30) training accuracy: 71.9808943548; validation accuracy: 72.3584211817\n",
      "Iteration 2400 / 23910) loss: 0.495290857315\n",
      "Iteration 2450 / 23910) loss: 0.591566405773\n",
      "Iteration 2500 / 23910) loss: 0.670653774261\n",
      "Iteration 2550 / 23910) loss: 0.603188301086\n",
      "Iteration 2600 / 23910) loss: 0.751729020119\n",
      "Iteration 2650 / 23910) loss: 0.58174601841\n",
      "Iteration 2700 / 23910) loss: 0.594353582382\n",
      "Iteration 2750 / 23910) loss: 0.540395477295\n",
      "Iteration 2800 / 23910) loss: 0.611037478924\n",
      "Iteration 2850 / 23910) loss: 0.695936856747\n",
      "Iteration 2900 / 23910) loss: 0.7565422225\n",
      "Iteration 2950 / 23910) loss: 0.745879460812\n",
      "Iteration 3000 / 23910) loss: 0.747733361244\n",
      "Iteration 3050 / 23910) loss: 0.495942668915\n",
      "Iteration 3100 / 23910) loss: 0.585423166275\n",
      "Iteration 3150 / 23910) loss: 0.688860370636\n",
      "(Epoch 4 / 30) training accuracy: 76.7209497662; validation accuracy: 76.9704584457\n",
      "Iteration 3200 / 23910) loss: 0.532212146759\n",
      "Iteration 3250 / 23910) loss: 0.65864953661\n",
      "Iteration 3300 / 23910) loss: 0.542224060059\n",
      "Iteration 3350 / 23910) loss: 0.748869366646\n",
      "Iteration 3400 / 23910) loss: 0.630244544983\n",
      "Iteration 3450 / 23910) loss: 0.719527516842\n",
      "Iteration 3500 / 23910) loss: 0.578568630219\n",
      "Iteration 3550 / 23910) loss: 0.550616707802\n",
      "Iteration 3600 / 23910) loss: 0.735995402336\n",
      "Iteration 3650 / 23910) loss: 0.615838867188\n",
      "Iteration 3700 / 23910) loss: 0.548821890831\n",
      "Iteration 3750 / 23910) loss: 0.514708720207\n",
      "Iteration 3800 / 23910) loss: 0.73461045742\n",
      "Iteration 3850 / 23910) loss: 0.473727494717\n",
      "Iteration 3900 / 23910) loss: 0.533981003761\n",
      "Iteration 3950 / 23910) loss: 0.600843617916\n",
      "(Epoch 5 / 30) training accuracy: 75.1325736207; validation accuracy: 75.2007232165\n",
      "Iteration 4000 / 23910) loss: 0.676547439098\n",
      "Iteration 4050 / 23910) loss: 0.475128386497\n",
      "Iteration 4100 / 23910) loss: 0.75484624052\n",
      "Iteration 4150 / 23910) loss: 0.47611611557\n",
      "Iteration 4200 / 23910) loss: 0.494221694946\n",
      "Iteration 4250 / 23910) loss: 0.644476560593\n",
      "Iteration 4300 / 23910) loss: 0.634474960327\n",
      "Iteration 4350 / 23910) loss: 0.559181733608\n",
      "Iteration 4400 / 23910) loss: 0.61998862505\n",
      "Iteration 4450 / 23910) loss: 0.577870256424\n",
      "Iteration 4500 / 23910) loss: 0.594236087799\n",
      "Iteration 4550 / 23910) loss: 0.604315202236\n",
      "Iteration 4600 / 23910) loss: 0.633276534081\n",
      "Iteration 4650 / 23910) loss: 0.676633091927\n",
      "Iteration 4700 / 23910) loss: 0.578449510574\n",
      "Iteration 4750 / 23910) loss: 0.483520278454\n",
      "(Epoch 6 / 30) training accuracy: 76.6708037158; validation accuracy: 76.9857808286\n",
      "Iteration 4800 / 23910) loss: 0.609728770256\n",
      "Iteration 4850 / 23910) loss: 0.555653122425\n",
      "Iteration 4900 / 23910) loss: 0.53470193243\n",
      "Iteration 4950 / 23910) loss: 0.446664477348\n",
      "Iteration 5000 / 23910) loss: 0.584753405571\n",
      "Iteration 5050 / 23910) loss: 0.565130488873\n",
      "Iteration 5100 / 23910) loss: 0.637763311386\n",
      "Iteration 5150 / 23910) loss: 0.695157289505\n",
      "Iteration 5200 / 23910) loss: 0.485540453911\n",
      "Iteration 5250 / 23910) loss: 0.661721654892\n",
      "Iteration 5300 / 23910) loss: 0.591050247669\n",
      "Iteration 5350 / 23910) loss: 0.556161868572\n",
      "Iteration 5400 / 23910) loss: 0.594479465485\n",
      "Iteration 5450 / 23910) loss: 0.511993918419\n",
      "Iteration 5500 / 23910) loss: 0.527288231373\n",
      "Iteration 5550 / 23910) loss: 0.65736213541\n",
      "(Epoch 7 / 30) training accuracy: 76.8977145938; validation accuracy: 77.2340034322\n",
      "Iteration 5600 / 23910) loss: 0.491882673264\n",
      "Iteration 5650 / 23910) loss: 0.512866205692\n",
      "Iteration 5700 / 23910) loss: 0.510953155041\n",
      "Iteration 5750 / 23910) loss: 0.528914139271\n",
      "Iteration 5800 / 23910) loss: 0.699046485901\n",
      "Iteration 5850 / 23910) loss: 0.506701012611\n",
      "Iteration 5900 / 23910) loss: 0.446350584507\n",
      "Iteration 5950 / 23910) loss: 0.558059606552\n",
      "Iteration 6000 / 23910) loss: 0.566885307312\n",
      "Iteration 6050 / 23910) loss: 0.574212182999\n",
      "Iteration 6100 / 23910) loss: 0.559020953178\n",
      "Iteration 6150 / 23910) loss: 0.563476007462\n",
      "Iteration 6200 / 23910) loss: 0.613093854904\n",
      "Iteration 6250 / 23910) loss: 0.529914416313\n",
      "Iteration 6300 / 23910) loss: 0.479455899715\n",
      "Iteration 6350 / 23910) loss: 0.511763132095\n",
      "(Epoch 8 / 30) training accuracy: 77.3778630261; validation accuracy: 77.5527089973\n",
      "Iteration 6400 / 23910) loss: 0.583191058159\n",
      "Iteration 6450 / 23910) loss: 0.71786935091\n",
      "Iteration 6500 / 23910) loss: 0.538573661804\n",
      "Iteration 6550 / 23910) loss: 0.636528609276\n",
      "Iteration 6600 / 23910) loss: 0.516154096603\n",
      "Iteration 6650 / 23910) loss: 0.561334424973\n",
      "Iteration 6700 / 23910) loss: 0.552004585266\n",
      "Iteration 6750 / 23910) loss: 0.528147598267\n",
      "Iteration 6800 / 23910) loss: 0.5599107337\n",
      "Iteration 6850 / 23910) loss: 0.551331787109\n",
      "Iteration 6900 / 23910) loss: 0.643753185272\n",
      "Iteration 6950 / 23910) loss: 0.590113679409\n",
      "Iteration 7000 / 23910) loss: 0.504569354534\n",
      "Iteration 7050 / 23910) loss: 0.555869401932\n",
      "Iteration 7100 / 23910) loss: 0.597521532536\n",
      "Iteration 7150 / 23910) loss: 0.637755367756\n",
      "(Epoch 9 / 30) training accuracy: 77.4292627277; validation accuracy: 77.7733513116\n",
      "Iteration 7200 / 23910) loss: 0.557037937641\n",
      "Iteration 7250 / 23910) loss: 0.505970962524\n",
      "Iteration 7300 / 23910) loss: 0.598518390179\n",
      "Iteration 7350 / 23910) loss: 0.510216183662\n",
      "Iteration 7400 / 23910) loss: 0.678805367947\n",
      "Iteration 7450 / 23910) loss: 0.525448887825\n",
      "Iteration 7500 / 23910) loss: 0.664204404354\n",
      "Iteration 7550 / 23910) loss: 0.458455207825\n",
      "Iteration 7600 / 23910) loss: 0.577354232788\n",
      "Iteration 7650 / 23910) loss: 0.50576903677\n",
      "Iteration 7700 / 23910) loss: 0.546432134628\n",
      "Iteration 7750 / 23910) loss: 0.539710607529\n",
      "Iteration 7800 / 23910) loss: 0.617129487991\n",
      "Iteration 7850 / 23910) loss: 0.574247683048\n",
      "Iteration 7900 / 23910) loss: 0.639228815556\n",
      "Iteration 7950 / 23910) loss: 0.502012904644\n",
      "(Epoch 10 / 30) training accuracy: 77.6850075846; validation accuracy: 77.7151262564\n",
      "Iteration 8000 / 23910) loss: 0.505652048111\n",
      "Iteration 8050 / 23910) loss: 0.566934880733\n",
      "Iteration 8100 / 23910) loss: 0.655049152374\n",
      "Iteration 8150 / 23910) loss: 0.617869423866\n",
      "Iteration 8200 / 23910) loss: 0.621427211761\n",
      "Iteration 8250 / 23910) loss: 0.562184345245\n",
      "Iteration 8300 / 23910) loss: 0.602010502815\n",
      "Iteration 8350 / 23910) loss: 0.494897082329\n",
      "Iteration 8400 / 23910) loss: 0.447601343155\n",
      "Iteration 8450 / 23910) loss: 0.554403356552\n",
      "Iteration 8500 / 23910) loss: 0.484639008522\n",
      "Iteration 8550 / 23910) loss: 0.52911004734\n",
      "Iteration 8600 / 23910) loss: 0.672394124985\n",
      "Iteration 8650 / 23910) loss: 0.511554779053\n",
      "Iteration 8700 / 23910) loss: 0.642393594742\n",
      "Iteration 8750 / 23910) loss: 0.501932907581\n",
      "(Epoch 11 / 30) training accuracy: 77.2487369464; validation accuracy: 77.2508580534\n",
      "Iteration 8800 / 23910) loss: 0.503779384613\n",
      "Iteration 8850 / 23910) loss: 0.519084630489\n",
      "Iteration 8900 / 23910) loss: 0.525080040932\n",
      "Iteration 8950 / 23910) loss: 0.499800388336\n",
      "Iteration 9000 / 23910) loss: 0.533302093983\n",
      "Iteration 9050 / 23910) loss: 0.45118222332\n",
      "Iteration 9100 / 23910) loss: 0.56732462883\n",
      "Iteration 9150 / 23910) loss: 0.64255943203\n",
      "Iteration 9200 / 23910) loss: 0.439982852936\n",
      "Iteration 9250 / 23910) loss: 0.509190971375\n",
      "Iteration 9300 / 23910) loss: 0.435663976669\n",
      "Iteration 9350 / 23910) loss: 0.625451288223\n",
      "Iteration 9400 / 23910) loss: 0.532551494122\n",
      "Iteration 9450 / 23910) loss: 0.544471569061\n",
      "Iteration 9500 / 23910) loss: 0.494307958603\n",
      "Iteration 9550 / 23910) loss: 0.670037444115\n",
      "(Epoch 12 / 30) training accuracy: 78.8922737473; validation accuracy: 79.2366388821\n",
      "Iteration 9600 / 23910) loss: 0.535404916763\n",
      "Iteration 9650 / 23910) loss: 0.541781498909\n",
      "Iteration 9700 / 23910) loss: 0.554488595963\n",
      "Iteration 9750 / 23910) loss: 0.612410337448\n",
      "Iteration 9800 / 23910) loss: 0.463607861519\n",
      "Iteration 9850 / 23910) loss: 0.49141660881\n",
      "Iteration 9900 / 23910) loss: 0.567388586044\n",
      "Iteration 9950 / 23910) loss: 0.533709669113\n",
      "Iteration 10000 / 23910) loss: 0.56397560215\n",
      "Iteration 10050 / 23910) loss: 0.645234224319\n",
      "Iteration 10100 / 23910) loss: 0.557695056915\n",
      "Iteration 10150 / 23910) loss: 0.580521504402\n",
      "Iteration 10200 / 23910) loss: 0.437412073135\n",
      "Iteration 10250 / 23910) loss: 0.537363420486\n",
      "Iteration 10300 / 23910) loss: 0.574875305176\n",
      "Iteration 10350 / 23910) loss: 0.581822216988\n",
      "(Epoch 13 / 30) training accuracy: 78.6816603357; validation accuracy: 78.8091443981\n",
      "Iteration 10400 / 23910) loss: 0.59498802948\n",
      "Iteration 10450 / 23910) loss: 0.580574318886\n",
      "Iteration 10500 / 23910) loss: 0.482634642601\n",
      "Iteration 10550 / 23910) loss: 0.565184046745\n",
      "Iteration 10600 / 23910) loss: 0.494591570854\n",
      "Iteration 10650 / 23910) loss: 0.56464440918\n",
      "Iteration 10700 / 23910) loss: 0.51030572319\n",
      "Iteration 10750 / 23910) loss: 0.5517482481\n",
      "Iteration 10800 / 23910) loss: 0.530255624771\n",
      "Iteration 10850 / 23910) loss: 0.502827778816\n",
      "Iteration 10900 / 23910) loss: 0.458147022247\n",
      "Iteration 10950 / 23910) loss: 0.503724539757\n",
      "Iteration 11000 / 23910) loss: 0.439814718246\n",
      "Iteration 11050 / 23910) loss: 0.434013101578\n",
      "Iteration 11100 / 23910) loss: 0.521616838455\n",
      "Iteration 11150 / 23910) loss: 0.568930007935\n",
      "(Epoch 14 / 30) training accuracy: 79.2783983352; validation accuracy: 79.4251041922\n",
      "Iteration 11200 / 23910) loss: 0.438529137611\n",
      "Iteration 11250 / 23910) loss: 0.420520936966\n",
      "Iteration 11300 / 23910) loss: 0.563901106834\n",
      "Iteration 11350 / 23910) loss: 0.456313336372\n",
      "Iteration 11400 / 23910) loss: 0.497607647896\n",
      "Iteration 11450 / 23910) loss: 0.528896801949\n",
      "Iteration 11500 / 23910) loss: 0.617128288269\n",
      "Iteration 11550 / 23910) loss: 0.457732971191\n",
      "Iteration 11600 / 23910) loss: 0.520003376007\n",
      "Iteration 11650 / 23910) loss: 0.427186877251\n",
      "Iteration 11700 / 23910) loss: 0.569696198463\n",
      "Iteration 11750 / 23910) loss: 0.50186974144\n",
      "Iteration 11800 / 23910) loss: 0.522336977959\n",
      "Iteration 11850 / 23910) loss: 0.472597530365\n",
      "Iteration 11900 / 23910) loss: 0.549751216888\n",
      "Iteration 11950 / 23910) loss: 0.451008886337\n",
      "(Epoch 15 / 30) training accuracy: 79.8713753808; validation accuracy: 80.0042902672\n",
      "Iteration 12000 / 23910) loss: 0.601668074608\n",
      "Iteration 12050 / 23910) loss: 0.426716099739\n",
      "Iteration 12100 / 23910) loss: 0.566312437057\n",
      "Iteration 12150 / 23910) loss: 0.577166730881\n",
      "Iteration 12200 / 23910) loss: 0.492538100243\n",
      "Iteration 12250 / 23910) loss: 0.621894442558\n",
      "Iteration 12300 / 23910) loss: 0.609768404007\n",
      "Iteration 12350 / 23910) loss: 0.439789859772\n",
      "Iteration 12400 / 23910) loss: 0.520684862137\n",
      "Iteration 12450 / 23910) loss: 0.565710059166\n",
      "Iteration 12500 / 23910) loss: 0.477191062927\n",
      "Iteration 12550 / 23910) loss: 0.590645518303\n",
      "Iteration 12600 / 23910) loss: 0.533562839508\n",
      "Iteration 12650 / 23910) loss: 0.489233041763\n",
      "Iteration 12700 / 23910) loss: 0.461203400612\n",
      "Iteration 12750 / 23910) loss: 0.525406909943\n",
      "(Epoch 16 / 30) training accuracy: 80.4919327541; validation accuracy: 80.7275067418\n",
      "Iteration 12800 / 23910) loss: 0.463820943832\n",
      "Iteration 12850 / 23910) loss: 0.453167537689\n",
      "Iteration 12900 / 23910) loss: 0.495466567039\n",
      "Iteration 12950 / 23910) loss: 0.582175949097\n",
      "Iteration 13000 / 23910) loss: 0.585441604614\n",
      "Iteration 13050 / 23910) loss: 0.582350572586\n",
      "Iteration 13100 / 23910) loss: 0.478592747688\n",
      "Iteration 13150 / 23910) loss: 0.515464556694\n",
      "Iteration 13200 / 23910) loss: 0.523306203842\n",
      "Iteration 13250 / 23910) loss: 0.446491506577\n",
      "Iteration 13300 / 23910) loss: 0.661126893997\n",
      "Iteration 13350 / 23910) loss: 0.47540216732\n",
      "Iteration 13400 / 23910) loss: 0.457162456512\n",
      "Iteration 13450 / 23910) loss: 0.502227387428\n",
      "Iteration 13500 / 23910) loss: 0.571759022713\n",
      "(Epoch 17 / 30) training accuracy: 80.1321348427; validation accuracy: 80.2463839176\n",
      "Iteration 13550 / 23910) loss: 0.490336644173\n",
      "Iteration 13600 / 23910) loss: 0.576206816673\n",
      "Iteration 13650 / 23910) loss: 0.475035167694\n",
      "Iteration 13700 / 23910) loss: 0.537879886627\n",
      "Iteration 13750 / 23910) loss: 0.550181565285\n",
      "Iteration 13800 / 23910) loss: 0.563695331573\n",
      "Iteration 13850 / 23910) loss: 0.562482403755\n",
      "Iteration 13900 / 23910) loss: 0.709395747185\n",
      "Iteration 13950 / 23910) loss: 0.469741219521\n",
      "Iteration 14000 / 23910) loss: 0.419967441559\n",
      "Iteration 14050 / 23910) loss: 0.475413900375\n",
      "Iteration 14100 / 23910) loss: 0.43118164444\n",
      "Iteration 14150 / 23910) loss: 0.415014656067\n",
      "Iteration 14200 / 23910) loss: 0.55767783165\n",
      "Iteration 14250 / 23910) loss: 0.582091473579\n",
      "Iteration 14300 / 23910) loss: 0.468073295593\n",
      "(Epoch 18 / 30) training accuracy: 80.4806498928; validation accuracy: 80.5834763422\n",
      "Iteration 14350 / 23910) loss: 0.708426218033\n",
      "Iteration 14400 / 23910) loss: 0.552213413239\n",
      "Iteration 14450 / 23910) loss: 0.476627241135\n",
      "Iteration 14500 / 23910) loss: 0.505966022491\n",
      "Iteration 14550 / 23910) loss: 0.557776999474\n",
      "Iteration 14600 / 23910) loss: 0.584325473785\n",
      "Iteration 14650 / 23910) loss: 0.481542254448\n",
      "Iteration 14700 / 23910) loss: 0.446295295715\n",
      "Iteration 14750 / 23910) loss: 0.558889222145\n",
      "Iteration 14800 / 23910) loss: 0.640547822952\n",
      "Iteration 14850 / 23910) loss: 0.618886815071\n",
      "Iteration 14900 / 23910) loss: 0.495846300125\n",
      "Iteration 14950 / 23910) loss: 0.513944080353\n",
      "Iteration 15000 / 23910) loss: 0.496037950516\n",
      "Iteration 15050 / 23910) loss: 0.55190222168\n",
      "Iteration 15100 / 23910) loss: 0.571992304802\n",
      "(Epoch 19 / 30) training accuracy: 79.693356902; validation accuracy: 79.8755822506\n",
      "Iteration 15150 / 23910) loss: 0.431344522476\n",
      "Iteration 15200 / 23910) loss: 0.513761800766\n",
      "Iteration 15250 / 23910) loss: 0.541036270142\n",
      "Iteration 15300 / 23910) loss: 0.549305337906\n",
      "Iteration 15350 / 23910) loss: 0.530468603134\n",
      "Iteration 15400 / 23910) loss: 0.483747793198\n",
      "Iteration 15450 / 23910) loss: 0.421844605446\n",
      "Iteration 15500 / 23910) loss: 0.670138932228\n",
      "Iteration 15550 / 23910) loss: 0.477469981194\n",
      "Iteration 15600 / 23910) loss: 0.517766757011\n",
      "Iteration 15650 / 23910) loss: 0.429302521706\n",
      "Iteration 15700 / 23910) loss: 0.46497294426\n",
      "Iteration 15750 / 23910) loss: 0.447395423889\n",
      "Iteration 15800 / 23910) loss: 0.474982495308\n",
      "Iteration 15850 / 23910) loss: 0.449675703049\n",
      "Iteration 15900 / 23910) loss: 0.474845671654\n",
      "(Epoch 20 / 30) training accuracy: 80.5884639011; validation accuracy: 80.6570237803\n",
      "Iteration 15950 / 23910) loss: 0.417478378296\n",
      "Iteration 16000 / 23910) loss: 0.465065293312\n",
      "Iteration 16050 / 23910) loss: 0.516506964684\n",
      "Iteration 16100 / 23910) loss: 0.566746149063\n",
      "Iteration 16150 / 23910) loss: 0.477515556335\n",
      "Iteration 16200 / 23910) loss: 0.540376174927\n",
      "Iteration 16250 / 23910) loss: 0.532210174561\n",
      "Iteration 16300 / 23910) loss: 0.557379525185\n",
      "Iteration 16350 / 23910) loss: 0.511824160576\n",
      "Iteration 16400 / 23910) loss: 0.559532416344\n",
      "Iteration 16450 / 23910) loss: 0.575381936073\n",
      "Iteration 16500 / 23910) loss: 0.526068733215\n",
      "Iteration 16550 / 23910) loss: 0.552585589409\n",
      "Iteration 16600 / 23910) loss: 0.462520217896\n",
      "Iteration 16650 / 23910) loss: 0.475267219543\n",
      "Iteration 16700 / 23910) loss: 0.363651621819\n",
      "(Epoch 21 / 30) training accuracy: 81.2428698585; validation accuracy: 81.3986271145\n",
      "Iteration 16750 / 23910) loss: 0.49455200386\n",
      "Iteration 16800 / 23910) loss: 0.519972632408\n",
      "Iteration 16850 / 23910) loss: 0.47658633709\n",
      "Iteration 16900 / 23910) loss: 0.439262433052\n",
      "Iteration 16950 / 23910) loss: 0.50535725975\n",
      "Iteration 17000 / 23910) loss: 0.404813613892\n",
      "Iteration 17050 / 23910) loss: 0.655452828407\n",
      "Iteration 17100 / 23910) loss: 0.559389808655\n",
      "Iteration 17150 / 23910) loss: 0.506147208214\n",
      "Iteration 17200 / 23910) loss: 0.448907793045\n",
      "Iteration 17250 / 23910) loss: 0.541333860397\n",
      "Iteration 17300 / 23910) loss: 0.596953243256\n",
      "Iteration 17350 / 23910) loss: 0.434966929436\n",
      "Iteration 17400 / 23910) loss: 0.408090341568\n",
      "Iteration 17450 / 23910) loss: 0.486907114983\n",
      "Iteration 17500 / 23910) loss: 0.545367206573\n",
      "(Epoch 22 / 30) training accuracy: 81.5349706019; validation accuracy: 81.6453174798\n",
      "Iteration 17550 / 23910) loss: 0.517437238693\n",
      "Iteration 17600 / 23910) loss: 0.501395622253\n",
      "Iteration 17650 / 23910) loss: 0.449656919479\n",
      "Iteration 17700 / 23910) loss: 0.512114735603\n",
      "Iteration 17750 / 23910) loss: 0.366152518272\n",
      "Iteration 17800 / 23910) loss: 0.470025559425\n",
      "Iteration 17850 / 23910) loss: 0.469811356544\n",
      "Iteration 17900 / 23910) loss: 0.492205757141\n",
      "Iteration 17950 / 23910) loss: 0.616009872437\n",
      "Iteration 18000 / 23910) loss: 0.467799778938\n",
      "Iteration 18050 / 23910) loss: 0.513379351616\n",
      "Iteration 18100 / 23910) loss: 0.474012622833\n",
      "Iteration 18150 / 23910) loss: 0.513818917274\n",
      "Iteration 18200 / 23910) loss: 0.481696777344\n",
      "Iteration 18250 / 23910) loss: 0.533010206223\n",
      "Iteration 18300 / 23910) loss: 0.561521290779\n",
      "(Epoch 23 / 30) training accuracy: 81.2942695601; validation accuracy: 81.4277396421\n",
      "Iteration 18350 / 23910) loss: 0.548314153671\n",
      "Iteration 18400 / 23910) loss: 0.433333683014\n",
      "Iteration 18450 / 23910) loss: 0.485617359161\n",
      "Iteration 18500 / 23910) loss: 0.51269534111\n",
      "Iteration 18550 / 23910) loss: 0.582370423317\n",
      "Iteration 18600 / 23910) loss: 0.533149418831\n",
      "Iteration 18650 / 23910) loss: 0.526147768974\n",
      "Iteration 18700 / 23910) loss: 0.46086293602\n",
      "Iteration 18750 / 23910) loss: 0.650564791679\n",
      "Iteration 18800 / 23910) loss: 0.54614926815\n",
      "Iteration 18850 / 23910) loss: 0.439714255333\n",
      "Iteration 18900 / 23910) loss: 0.488057693481\n",
      "Iteration 18950 / 23910) loss: 0.510091449738\n",
      "Iteration 19000 / 23910) loss: 0.524538106918\n",
      "Iteration 19050 / 23910) loss: 0.463731281281\n",
      "Iteration 19100 / 23910) loss: 0.447254109383\n",
      "(Epoch 24 / 30) training accuracy: 81.5963995136; validation accuracy: 81.6637043393\n",
      "Iteration 19150 / 23910) loss: 0.490429899216\n",
      "Iteration 19200 / 23910) loss: 0.541882941246\n",
      "Iteration 19250 / 23910) loss: 0.557962985992\n",
      "Iteration 19300 / 23910) loss: 0.48859612751\n",
      "Iteration 19350 / 23910) loss: 0.41505563736\n",
      "Iteration 19400 / 23910) loss: 0.50849331665\n",
      "Iteration 19450 / 23910) loss: 0.567157914162\n",
      "Iteration 19500 / 23910) loss: 0.561605854034\n",
      "Iteration 19550 / 23910) loss: 0.536817392349\n",
      "Iteration 19600 / 23910) loss: 0.36587412262\n",
      "Iteration 19650 / 23910) loss: 0.410264704704\n",
      "Iteration 19700 / 23910) loss: 0.489880249023\n",
      "Iteration 19750 / 23910) loss: 0.419918737411\n",
      "Iteration 19800 / 23910) loss: 0.518764354706\n",
      "Iteration 19850 / 23910) loss: 0.444703364372\n",
      "Iteration 19900 / 23910) loss: 0.544838603973\n",
      "(Epoch 25 / 30) training accuracy: 81.8997831183; validation accuracy: 81.8460406962\n",
      "Iteration 19950 / 23910) loss: 0.521260311127\n",
      "Iteration 20000 / 23910) loss: 0.631253748894\n",
      "Iteration 20050 / 23910) loss: 0.586648860931\n",
      "Iteration 20100 / 23910) loss: 0.407074335098\n",
      "Iteration 20150 / 23910) loss: 0.462322528839\n",
      "Iteration 20200 / 23910) loss: 0.550010778427\n",
      "Iteration 20250 / 23910) loss: 0.434045369148\n",
      "Iteration 20300 / 23910) loss: 0.619049283981\n",
      "Iteration 20350 / 23910) loss: 0.463589614868\n",
      "Iteration 20400 / 23910) loss: 0.475381562233\n",
      "Iteration 20450 / 23910) loss: 0.5083524189\n",
      "Iteration 20500 / 23910) loss: 0.581414792061\n",
      "Iteration 20550 / 23910) loss: 0.489890895844\n",
      "Iteration 20600 / 23910) loss: 0.447398618698\n",
      "Iteration 20650 / 23910) loss: 0.554114701271\n",
      "Iteration 20700 / 23910) loss: 0.623185806274\n",
      "(Epoch 26 / 30) training accuracy: 81.656574774; validation accuracy: 81.6698332925\n",
      "Iteration 20750 / 23910) loss: 0.466083931923\n",
      "Iteration 20800 / 23910) loss: 0.529988251686\n",
      "Iteration 20850 / 23910) loss: 0.621024786949\n",
      "Iteration 20900 / 23910) loss: 0.466567940712\n",
      "Iteration 20950 / 23910) loss: 0.618909903526\n",
      "Iteration 21000 / 23910) loss: 0.394126032829\n",
      "Iteration 21050 / 23910) loss: 0.546985657692\n",
      "Iteration 21100 / 23910) loss: 0.536287700653\n",
      "Iteration 21150 / 23910) loss: 0.434337663651\n",
      "Iteration 21200 / 23910) loss: 0.508442458153\n",
      "Iteration 21250 / 23910) loss: 0.463173633575\n",
      "Iteration 21300 / 23910) loss: 0.457084701538\n",
      "Iteration 21350 / 23910) loss: 0.440650613785\n",
      "Iteration 21400 / 23910) loss: 0.468534118652\n",
      "Iteration 21450 / 23910) loss: 0.466512992859\n",
      "Iteration 21500 / 23910) loss: 0.452412016869\n",
      "(Epoch 27 / 30) training accuracy: 81.8960221646; validation accuracy: 81.7617675901\n",
      "Iteration 21550 / 23910) loss: 0.528481677055\n",
      "Iteration 21600 / 23910) loss: 0.536590735435\n",
      "Iteration 21650 / 23910) loss: 0.561606649399\n",
      "Iteration 21700 / 23910) loss: 0.404972477913\n",
      "Iteration 21750 / 23910) loss: 0.442096683502\n",
      "Iteration 21800 / 23910) loss: 0.522122268677\n",
      "Iteration 21850 / 23910) loss: 0.477639883041\n",
      "Iteration 21900 / 23910) loss: 0.50938627243\n",
      "Iteration 21950 / 23910) loss: 0.635717718124\n",
      "Iteration 22000 / 23910) loss: 0.397725073814\n",
      "Iteration 22050 / 23910) loss: 0.445554955482\n",
      "Iteration 22100 / 23910) loss: 0.53539700985\n",
      "Iteration 22150 / 23910) loss: 0.54508191967\n",
      "Iteration 22200 / 23910) loss: 0.472112868309\n",
      "Iteration 22250 / 23910) loss: 0.452322151184\n",
      "Iteration 22300 / 23910) loss: 0.472717100143\n",
      "(Epoch 28 / 30) training accuracy: 82.2696102398; validation accuracy: 82.2505516058\n",
      "Iteration 22350 / 23910) loss: 0.489640517235\n",
      "Iteration 22400 / 23910) loss: 0.608183444977\n",
      "Iteration 22450 / 23910) loss: 0.50811713028\n",
      "Iteration 22500 / 23910) loss: 0.548191333771\n",
      "Iteration 22550 / 23910) loss: 0.409188035965\n",
      "Iteration 22600 / 23910) loss: 0.529475011826\n",
      "Iteration 22650 / 23910) loss: 0.464512845993\n",
      "Iteration 22700 / 23910) loss: 0.496309054375\n",
      "Iteration 22750 / 23910) loss: 0.451479204178\n",
      "Iteration 22800 / 23910) loss: 0.484740642548\n",
      "Iteration 22850 / 23910) loss: 0.579487665176\n",
      "Iteration 22900 / 23910) loss: 0.452844161034\n",
      "Iteration 22950 / 23910) loss: 0.550910881042\n",
      "Iteration 23000 / 23910) loss: 0.531972388268\n",
      "Iteration 23050 / 23910) loss: 0.578684751511\n",
      "Iteration 23100 / 23910) loss: 0.510170482635\n",
      "(Epoch 29 / 30) training accuracy: 82.1492597189; validation accuracy: 82.2061166953\n",
      "Iteration 23150 / 23910) loss: 0.397770269394\n",
      "Iteration 23200 / 23910) loss: 0.493845188141\n",
      "Iteration 23250 / 23910) loss: 0.491706198692\n",
      "Iteration 23300 / 23910) loss: 0.468956863403\n",
      "Iteration 23350 / 23910) loss: 0.534504706383\n",
      "Iteration 23400 / 23910) loss: 0.384894908905\n",
      "Iteration 23450 / 23910) loss: 0.528516391754\n",
      "Iteration 23500 / 23910) loss: 0.486418581963\n",
      "Iteration 23550 / 23910) loss: 0.537869503975\n",
      "Iteration 23600 / 23910) loss: 0.547769877434\n",
      "Iteration 23650 / 23910) loss: 0.514723705292\n",
      "Iteration 23700 / 23910) loss: 0.447103792191\n",
      "Iteration 23750 / 23910) loss: 0.475862644196\n",
      "Iteration 23800 / 23910) loss: 0.596664572716\n",
      "Iteration 23850 / 23910) loss: 0.434051794052\n",
      "Iteration 23900 / 23910) loss: 0.546893386841\n",
      "(Epoch 30 / 30) training accuracy: 82.28966866; validation accuracy: 82.241358176\n",
      "The best validation accuracy:  82.2505516058\n",
      "The corresponding training accuracy:  82.2696102398\n"
     ]
    }
   ],
   "source": [
    "#Training a neural network\n",
    "data_train = {}\n",
    "data_train['Xtrain']= Xtr\n",
    "data_train['Ytrain']=Ytr\n",
    "data_val = {}\n",
    "data_val['Xval']= Xv\n",
    "data_val['Yval']=Yv\n",
    "input_dims = Xtr.shape[1]\n",
    "\n",
    "#Dictionary of optional arguments to be passed to trainig method of NeuralNetwork class\n",
    "dict1 = {}\n",
    "dict1['num_epochs'] = 30 #number of epochs\n",
    "dict1['update_method'] = 'adam' #stochastic gradient-based update method used\n",
    "dict1['print_rate'] = 50 #printing rate of loss values\n",
    "hyperparamaters ={}\n",
    "hyperparamaters['learning_rate'] = 0.03\n",
    "dict1['rate_decay']= 0.90\n",
    "dict1['update_hyperparams'] = hyperparamaters\n",
    "dict1['batch_size']= 100\n",
    "reg = 0.001\n",
    "#regularization constant is set \n",
    "model2 = NeuralNetwork(input_dims,[200, 70, 30],5,reg) \n",
    "\n",
    "#Training. Will take awhile.\n",
    "model2.stoch_train(data_train, data_val, **dict1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After 30 epochs of training,we obtain parameters corresponding to a training and validation accuracy around 82%. To put this in perspective, in the paper \"Comparative Accuracies of Artificial Neural Networks and Discriminant Analysis in Predicting Forest Cover Types from Cartographic Variables\", they were able to obtain a test accuracy of about 70% by using a small training set and only using 12 of the features. We could obtain better accuracy by letting the model train for a longer period of time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
